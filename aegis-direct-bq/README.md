# Aegis BigQuery Ingestion Service

This repository contains a backend service for the **Aegis** digital well-being monitoring application. The service is a data ingestion API built with **FastAPI** that receives data from client applications and streams it directly into a **Google BigQuery** table.

This architecture provides a straightforward and efficient way to ingest large volumes of structured data for subsequent analysis.

-----

## üöÄ Features

  * **FastAPI Backend:** A lightweight and scalable API for receiving data.
  * **Pydantic Validation:** Ensures that all incoming data conforms to a predefined schema, preventing bad data from entering your pipeline.
  * **Direct BigQuery Streaming:** Streams data in real-time to a specified BigQuery table, eliminating the need for intermediate storage or complex pipelines for simple ingestion.
  * **Cross-Origin Resource Sharing (CORS):** Configured to accept requests from a Chrome extension, allowing for seamless integration.

-----

## üèÉ How to Run the Application

### Prerequisites

Before you begin, ensure you have the following tools and services configured:

  * **Python 3.8+**
  * **pip** (Python package installer)
  * A **Google Cloud Project** with billing enabled.
  * The **Google Cloud SDK** installed and authenticated on your local machine.
  * The **BigQuery API** enabled in your Google Cloud Project.
  * A Service Account with the `roles/bigquery.dataEditor` role to allow data insertion.

### 1\. Installation

First, clone the repository and install the required Python packages using the `requirements.txt` file.

```bash
pip install -r requirements.txt
```

The `requirements.txt` file should include the following:

```
fastapi
google-cloud-bigquery
pydantic
python-dotenv
uvicorn
```

### 2\. Setup and Configuration

Create a `.env` file in the root directory of your project. This file will store your Google Cloud configuration.

```env
# Replace the placeholder values with your own.
GCP_PROJECT_ID=[YOUR_GCP_PROJECT_ID]
BIGQUERY_DATASET_ID=[YOUR_BIGQUERY_DATASET_ID]
BIGQUERY_TABLE_ID=[YOUR_BIGQUERY_TABLE_ID]
```

### 3\. Run the Application

Execute the following command to start the FastAPI server using `uvicorn`.

```bash
uvicorn main:app --reload
```

The server will be available at `http://127.0.0.1:8000`. You can now send POST requests to the `/upload_data` endpoint.

-----

## üìå API Endpoint

### `POST /upload_data`

This endpoint accepts a list of JSON objects and streams them directly to the configured BigQuery table.

  * **Description:** Receives and processes a batch of user activity records.
  * **Request Body:** A JSON array of `SignalData` objects.

**Example Request:**

```json
[
  {
    "user_id": "user123",
    "timestamp": "2025-08-22T12:00:00Z",
    "signal_type": "NEUTRAL_FLAG",
    "flag_type": "NEUTRAL",
    "confidence": 0.95,
    "topic_category": "Educational Content",
    "source_platform": "Chrome Extension",
    "event_details": {
      "context": "Watching a video on historical events",
      "corroborating_signals": ["search for 'WWII documentaries'"]
    }
  },
  {
    "user_id": "user456",
    "timestamp": "2025-08-22T12:05:00Z",
    "signal_type": "IMMEDIATE_FLAG",
    "flag_type": "VIOLENCE",
    "confidence": 0.98,
    "topic_category": "Dangerous Behavior",
    "source_platform": "Chrome Extension",
    "event_details": {
      "context": "Accessing a forum discussing illegal weapons",
      "corroborating_signals": ["searched for 'gun parts online'", "visited a dark web marketplace"]
    }
  }
]
```